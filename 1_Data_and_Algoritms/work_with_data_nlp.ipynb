{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with data\n",
    "\n",
    "- Natural Language Processing (Text)\n",
    "- Computer Vision (Images)\n",
    "- Speech Processing (Voice)\n",
    "- Music Processing (Audio)\n",
    "- Time Series\n",
    "- Mixed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "- Preprocessing\n",
    "- Tokenization\n",
    "- Vectorization\n",
    "- Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB Dataset: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews#IMDB%20Dataset.csv\n",
    "\n",
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "with open('data/imdb/imdb_dataset.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    next(reader) #skip csv header\n",
    "    for row in reader:\n",
    "        samples += [row[0]]\n",
    "        labels += [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was the worst movie I saw at WorldFest and it also received the least amount of applause afterwards! I can only think it is receiving such recognition based on the amount of known actors in the film. It's great to see J.Beals but she's only in the movie for a few minutes. M.Parker is a much better actress than the part allowed for. The rest of the acting is hard to judge because the movie is so ridiculous and predictable. The main character is totally unsympathetic and therefore a bore to watch. There is no real emotional depth to the story. A movie revolving about an actor who can't get work doesn't feel very original to me. Nor does the development of the cop. It feels like one of many straight-to-video movies I saw back in the 90s ... And not even a good one in those standards.<br /><br />, negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"{samples[24]}, {labels[24]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "preprocessed_samples = []\n",
    "for sample in samples:\n",
    "    s = sample.lower()\n",
    "    s = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", s)\n",
    "    s = re.sub(\"\\s+\", \" \", s)\n",
    "    s = s.strip()\n",
    "    preprocessed_samples += [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this was the worst movie i saw at worldfest and it also received the least amount of applause afterwards i can only think it is receiving such recognition based on the amount of known actors in the film it s great to see j beals but she s only in the movie for a few minutes m parker is a much better actress than the part allowed for the rest of the acting is hard to judge because the movie is so ridiculous and predictable the main character is totally unsympathetic and therefore a bore to watch there is no real emotional depth to the story a movie revolving about an actor who can t get work doesn t feel very original to me nor does the development of the cop it feels like one of many straight to video movies i saw back in the 90s and not even a good one in those standards br br, negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"{preprocessed_samples[24]}, {labels[24]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples = []\n",
    "for sample in preprocessed_samples:\n",
    "    s = sample.split()\n",
    "    tokenized_samples += [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'was', 'the', 'worst', 'movie', 'i', 'saw', 'at', 'worldfest', 'and', 'it', 'also', 'received', 'the', 'least', 'amount', 'of', 'applause', 'afterwards', 'i', 'can', 'only', 'think', 'it', 'is', 'receiving', 'such', 'recognition', 'based', 'on', 'the', 'amount', 'of', 'known', 'actors', 'in', 'the', 'film', 'it', 's', 'great', 'to', 'see', 'j', 'beals', 'but', 'she', 's', 'only', 'in', 'the', 'movie', 'for', 'a', 'few', 'minutes', 'm', 'parker', 'is', 'a', 'much', 'better', 'actress', 'than', 'the', 'part', 'allowed', 'for', 'the', 'rest', 'of', 'the', 'acting', 'is', 'hard', 'to', 'judge', 'because', 'the', 'movie', 'is', 'so', 'ridiculous', 'and', 'predictable', 'the', 'main', 'character', 'is', 'totally', 'unsympathetic', 'and', 'therefore', 'a', 'bore', 'to', 'watch', 'there', 'is', 'no', 'real', 'emotional', 'depth', 'to', 'the', 'story', 'a', 'movie', 'revolving', 'about', 'an', 'actor', 'who', 'can', 't', 'get', 'work', 'doesn', 't', 'feel', 'very', 'original', 'to', 'me', 'nor', 'does', 'the', 'development', 'of', 'the', 'cop', 'it', 'feels', 'like', 'one', 'of', 'many', 'straight', 'to', 'video', 'movies', 'i', 'saw', 'back', 'in', 'the', '90s', 'and', 'not', 'even', 'a', 'good', 'one', 'in', 'those', 'standards', 'br', 'br'], negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tokenized_samples[24]}, {labels[24]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples_char = []\n",
    "for sample in preprocessed_samples:\n",
    "    s = list(sample)\n",
    "    tokenized_samples_char += [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'i', 's', ' ', 'w', 'a', 's', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 's', 't', ' ', 'm', 'o', 'v', 'i', 'e', ' ', 'i', ' ', 's', 'a', 'w', ' ', 'a', 't', ' ', 'w', 'o', 'r', 'l', 'd', 'f', 'e', 's', 't', ' ', 'a', 'n', 'd', ' ', 'i', 't', ' ', 'a', 'l', 's', 'o', ' ', 'r', 'e', 'c', 'e', 'i', 'v', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'l', 'e', 'a', 's', 't', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'a', 'p', 'p', 'l', 'a', 'u', 's', 'e', ' ', 'a', 'f', 't', 'e', 'r', 'w', 'a', 'r', 'd', 's', ' ', 'i', ' ', 'c', 'a', 'n', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', ' ', 'i', 's', ' ', 'r', 'e', 'c', 'e', 'i', 'v', 'i', 'n', 'g', ' ', 's', 'u', 'c', 'h', ' ', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'k', 'n', 'o', 'w', 'n', ' ', 'a', 'c', 't', 'o', 'r', 's', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'f', 'i', 'l', 'm', ' ', 'i', 't', ' ', 's', ' ', 'g', 'r', 'e', 'a', 't', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'j', ' ', 'b', 'e', 'a', 'l', 's', ' ', 'b', 'u', 't', ' ', 's', 'h', 'e', ' ', 's', ' ', 'o', 'n', 'l', 'y', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'v', 'i', 'e', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'f', 'e', 'w', ' ', 'm', 'i', 'n', 'u', 't', 'e', 's', ' ', 'm', ' ', 'p', 'a', 'r', 'k', 'e', 'r', ' ', 'i', 's', ' ', 'a', ' ', 'm', 'u', 'c', 'h', ' ', 'b', 'e', 't', 't', 'e', 'r', ' ', 'a', 'c', 't', 'r', 'e', 's', 's', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'a', 'r', 't', ' ', 'a', 'l', 'l', 'o', 'w', 'e', 'd', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'r', 'e', 's', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'c', 't', 'i', 'n', 'g', ' ', 'i', 's', ' ', 'h', 'a', 'r', 'd', ' ', 't', 'o', ' ', 'j', 'u', 'd', 'g', 'e', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'v', 'i', 'e', ' ', 'i', 's', ' ', 's', 'o', ' ', 'r', 'i', 'd', 'i', 'c', 'u', 'l', 'o', 'u', 's', ' ', 'a', 'n', 'd', ' ', 'p', 'r', 'e', 'd', 'i', 'c', 't', 'a', 'b', 'l', 'e', ' ', 't', 'h', 'e', ' ', 'm', 'a', 'i', 'n', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', ' ', 'i', 's', ' ', 't', 'o', 't', 'a', 'l', 'l', 'y', ' ', 'u', 'n', 's', 'y', 'm', 'p', 'a', 't', 'h', 'e', 't', 'i', 'c', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'r', 'e', 'f', 'o', 'r', 'e', ' ', 'a', ' ', 'b', 'o', 'r', 'e', ' ', 't', 'o', ' ', 'w', 'a', 't', 'c', 'h', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'n', 'o', ' ', 'r', 'e', 'a', 'l', ' ', 'e', 'm', 'o', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'd', 'e', 'p', 't', 'h', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 's', 't', 'o', 'r', 'y', ' ', 'a', ' ', 'm', 'o', 'v', 'i', 'e', ' ', 'r', 'e', 'v', 'o', 'l', 'v', 'i', 'n', 'g', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'a', 'n', ' ', 'a', 'c', 't', 'o', 'r', ' ', 'w', 'h', 'o', ' ', 'c', 'a', 'n', ' ', 't', ' ', 'g', 'e', 't', ' ', 'w', 'o', 'r', 'k', ' ', 'd', 'o', 'e', 's', 'n', ' ', 't', ' ', 'f', 'e', 'e', 'l', ' ', 'v', 'e', 'r', 'y', ' ', 'o', 'r', 'i', 'g', 'i', 'n', 'a', 'l', ' ', 't', 'o', ' ', 'm', 'e', ' ', 'n', 'o', 'r', ' ', 'd', 'o', 'e', 's', ' ', 't', 'h', 'e', ' ', 'd', 'e', 'v', 'e', 'l', 'o', 'p', 'm', 'e', 'n', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'p', ' ', 'i', 't', ' ', 'f', 'e', 'e', 'l', 's', ' ', 'l', 'i', 'k', 'e', ' ', 'o', 'n', 'e', ' ', 'o', 'f', ' ', 'm', 'a', 'n', 'y', ' ', 's', 't', 'r', 'a', 'i', 'g', 'h', 't', ' ', 't', 'o', ' ', 'v', 'i', 'd', 'e', 'o', ' ', 'm', 'o', 'v', 'i', 'e', 's', ' ', 'i', ' ', 's', 'a', 'w', ' ', 'b', 'a', 'c', 'k', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', '9', '0', 's', ' ', 'a', 'n', 'd', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'o', 'n', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'o', 's', 'e', ' ', 's', 't', 'a', 'n', 'd', 'a', 'r', 'd', 's', ' ', 'b', 'r', ' ', 'b', 'r'], negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tokenized_samples_char[24]}, {labels[24]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2word = []\n",
    "\n",
    "for sample in tokenized_samples:\n",
    "    for token in sample:\n",
    "        if token not in word2id.keys():\n",
    "            word2id[token] = len(id2word)\n",
    "            id2word += [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17933"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credits - 666\n"
     ]
    }
   ],
   "source": [
    "print(f\"{id2word[666]} - {word2id['credits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized_samples = []\n",
    "for sample in tokenized_samples:\n",
    "    s = [word2id[token] for token in sample]\n",
    "    digitized_samples += [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 34, 2, 649, 357, 117, 141, 317, 1358, 37, 66, 621, 1359, 2, 672, 1360, 1, 1361, 1362, 117, 185, 214, 1098, 66, 23, 1363, 627, 1122, 690, 77, 2, 1360, 1, 1364, 209, 43, 2, 368, 66, 238, 236, 60, 222, 1365, 1366, 146, 326, 238, 214, 43, 2, 357, 51, 49, 512, 894, 930, 1367, 23, 49, 744, 711, 1368, 248, 2, 1369, 1370, 51, 2, 476, 1, 2, 463, 23, 616, 60, 1371, 902, 2, 357, 23, 92, 962, 37, 748, 2, 120, 1002, 23, 379, 1372, 37, 1373, 49, 1374, 60, 400, 348, 23, 57, 377, 1031, 1375, 60, 2, 489, 49, 357, 1376, 33, 80, 1095, 157, 185, 127, 164, 485, 137, 127, 1001, 195, 620, 60, 28, 1377, 923, 2, 1027, 1, 2, 1378, 66, 1379, 376, 0, 1, 98, 1290, 60, 1380, 568, 117, 141, 543, 43, 2, 1381, 37, 48, 289, 49, 464, 0, 43, 687, 1382, 29, 29], negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"{digitized_samples[24]}, {labels[24]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'was', 'the', 'worst', 'movie', 'i', 'saw', 'at', 'worldfest', 'and', 'it', 'also', 'received', 'the', 'least', 'amount', 'of', 'applause', 'afterwards', 'i', 'can', 'only', 'think', 'it', 'is', 'receiving', 'such', 'recognition', 'based', 'on', 'the', 'amount', 'of', 'known', 'actors', 'in', 'the', 'film', 'it', 's', 'great', 'to', 'see', 'j', 'beals', 'but', 'she', 's', 'only', 'in', 'the', 'movie', 'for', 'a', 'few', 'minutes', 'm', 'parker', 'is', 'a', 'much', 'better', 'actress', 'than', 'the', 'part', 'allowed', 'for', 'the', 'rest', 'of', 'the', 'acting', 'is', 'hard', 'to', 'judge', 'because', 'the', 'movie', 'is', 'so', 'ridiculous', 'and', 'predictable', 'the', 'main', 'character', 'is', 'totally', 'unsympathetic', 'and', 'therefore', 'a', 'bore', 'to', 'watch', 'there', 'is', 'no', 'real', 'emotional', 'depth', 'to', 'the', 'story', 'a', 'movie', 'revolving', 'about', 'an', 'actor', 'who', 'can', 't', 'get', 'work', 'doesn', 't', 'feel', 'very', 'original', 'to', 'me', 'nor', 'does', 'the', 'development', 'of', 'the', 'cop', 'it', 'feels', 'like', 'one', 'of', 'many', 'straight', 'to', 'video', 'movies', 'i', 'saw', 'back', 'in', 'the', '90s', 'and', 'not', 'even', 'a', 'good', 'one', 'in', 'those', 'standards', 'br', 'br'], negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"{[id2word[index] for index in digitized_samples[24]]}, {labels[24]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_samples = []\n",
    "max_len = 64\n",
    "\n",
    "for sample in digitized_samples:\n",
    "    if len(sample) < max_len:\n",
    "        sample += [0] * (max_len - len(sample))\n",
    "    \n",
    "    correct_samples += [sample[:max_len]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_samples[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 34, 2, 649, 357, 117, 141, 317, 1358, 37, 66, 621, 1359, 2, 672, 1360, 1, 1361, 1362, 117, 185, 214, 1098, 66, 23, 1363, 627, 1122, 690, 77, 2, 1360, 1, 1364, 209, 43, 2, 368, 66, 238, 236, 60, 222, 1365, 1366, 146, 326, 238, 214, 43, 2, 357, 51, 49, 512, 894, 930, 1367, 23, 49, 744, 711, 1368, 248], negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"{correct_samples[24]}, {labels[24]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_samples[968])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 23, 49, 236, 368, 4348, 37, 3576, 2, 466, 23, 2117, 1920, 17635, 464, 485, 60, 2, 4074, 117, 1001, 92, 3704, 51, 17636, 226, 2, 2562, 1, 2563, 46, 14, 54, 117, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], positive\n"
     ]
    }
   ],
   "source": [
    "print(f\"{correct_samples[968]}, {labels[968]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels (digitize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "correct_labels = [labels_dict[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 34, 2, 649, 357, 117, 141, 317, 1358, 37, 66, 621, 1359, 2, 672, 1360, 1, 1361, 1362, 117, 185, 214, 1098, 66, 23, 1363, 627, 1122, 690, 77, 2, 1360, 1, 1364, 209, 43, 2, 368, 66, 238, 236, 60, 222, 1365, 1366, 146, 326, 238, 214, 43, 2, 357, 51, 49, 512, 894, 930, 1367, 23, 49, 744, 711, 1368, 248], 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"{correct_samples[24]}, {correct_labels[24]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 23, 49, 236, 368, 4348, 37, 3576, 2, 466, 23, 2117, 1920, 17635, 464, 485, 60, 2, 4074, 117, 1001, 92, 3704, 51, 17636, 226, 2, 2562, 1, 2563, 46, 14, 54, 117, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{correct_samples[968]}, {correct_labels[968]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = correct_samples[:666]\n",
    "train_labels = correct_labels[:666]\n",
    "test_data = correct_samples[666:]\n",
    "test_labels = correct_labels[666:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
