{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file=\"data/imdb/imdb_dataset.csv\", sg=0, min_count=5, size=100, window=4, negative=5, hs=0, sample=1e-5, workers=12, iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"emb_dir\"):\n",
    "    os.mkdir(\"emb_dir\") \n",
    "\n",
    "model.save(\"emb_dir/emb.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(\"emb_dir/emb.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"emb_dir/vocab.txt\", \"w\", encoding='utf-8') as f:\n",
    "    vocab = sorted([(model.wv.vocab[x].index, x, model.wv.vocab[x].count) for x in model.wv.vocab.keys()], key=lambda x: x[0])\n",
    "    vocab = [f\"{v[1]} {v[2]}\" for v in vocab]\n",
    "    vocab = \"\\n\".join(vocab)\n",
    "    f.write(vocab)\n",
    "\n",
    "np.save(\"emb_dir/emb\", model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [v.split()[0] for v in open(\"emb_dir/vocab.txt\").read().split(\"\\n\")]\n",
    "emb = np.load(\"emb_dir/emb.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25940"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25940, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859541"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(emb[vocab.index(\"green\")], emb[vocab.index(\"red\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[11123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99732995"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(emb[vocab.index(\"green\")], emb[11123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RusVectōrēs](https://rusvectores.org/ru/models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "could not find MARK",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a58757ff0eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pretrained/model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \"\"\"\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: could not find MARK"
     ]
    }
   ],
   "source": [
    "w2v_p = Word2Vec.load(\"pretrained/model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_p = open(\"pretrained/model.txt\").read().split(\"\\n\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189193"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'быть_VERB 0.56827927 -0.3209115 -0.019124394 -1.9356475 -0.8124491 1.657973 -1.3159128 1.1186373 -2.0223322 -3.2186239 -0.8087832 -1.6359693 0.30781016 -1.9637547 1.1508301 0.09154594 1.499977 -1.9233339 -1.395698 0.4522272 1.3875449 -0.6991994 1.1241058 -0.8872212 1.5669478 0.105251916 -0.72891355 -1.7446259 -0.39187282 -0.21783888 0.99993414 -2.8352168 0.28585172 -1.2888701 -0.7390094 0.7050435 2.6338942 0.37333074 2.078776 -0.19290456 -0.73848265 -1.7742666 1.6497376 3.7614036 -1.2978705 -0.00728586 1.3725481 -1.5491337 -0.81334364 -0.7748071 -0.61844414 0.7076229 0.53189814 -0.519196 1.2863954 0.99337226 1.1154282 -0.930117 1.572236 -1.2262355 -0.86334157 0.61980397 -0.09515494 0.5440264 -0.50054914 0.32205996 -1.49439 0.63581914 2.5604725 0.62158006 -0.9675487 1.7475013 -1.055982 1.8056836 0.29194352 0.72323376 -0.2652101 -1.5487951 -1.2479311 0.6863535 -0.5887072 -0.25386146 -1.0167173 -2.9647908 -0.96875453 -1.6287156 -0.60758674 -0.5515135 -0.4176201 0.6098959 -2.8192685 2.5853198 0.85308856 0.87172407 -0.897581 -0.40489018 1.4343278 1.2818788 -0.1038449 3.5178578 3.2740445 -0.8202849 -1.8105402 0.011025606 0.237255 -4.524287 -2.5001287 1.012236 0.9220278 0.08768477 1.7009947 -0.1830261 0.27147695 2.9195416 0.801868 -0.21166666 1.8762391 2.0312402 -0.83916956 -1.8924448 -0.8591967 1.3861572 1.4221402 -0.22199516 0.88558954 -2.7754016 0.76693803 -0.039527435 0.74935645 1.3386279 -0.39080724 -0.26873833 0.8972396 -0.60566366 -0.8532991 0.014365448 -0.67513114 -0.52097553 1.1179368 -1.1936499 -0.6559193 0.938022 -2.2571654 -3.0508792 -0.7227019 1.2745616 1.0202187 0.9840875 -2.8626523 1.2382461 1.1355385 0.0136938635 2.0628734 0.108944796 -0.86612064 -0.8116914 0.9342249 -0.8987377 0.011308536 0.80704564 2.0953562 -1.9135808 -0.5009951 -0.22260208 0.8624965 0.5630634 -0.46424702 -1.6521688 0.34720603 -1.36089 -0.21586478 1.9220291 0.46534094 1.4874742 -1.329455 2.045918 2.821619 1.9280642 -1.1321975 0.9344899 -0.7038843 0.7139956 -0.3580236 0.30644202 1.1503032 0.18564531 -2.0370424 1.7469863 -0.19078876 -0.645339 0.505186 -0.60695523 1.1013697 -0.57632995 -0.22718555 2.282853 2.5692198 1.8646836 1.0998862 -1.6795722 0.16015984 0.37569702 1.9649556 2.360805 -1.6617944 -1.0738456 -1.0985037 1.4406085 -1.5327767 -0.13647187 1.3672692 -0.15450615 -0.4382513 -3.0015786 -2.6966953 0.46445477 -0.117575265 -1.1011763 -0.3542566 -0.5057304 -0.42879865 0.32639202 -0.18921247 1.5726771 -0.7151896 -1.1668553 -0.42700508 -1.1397042 1.5330937 -3.2055368 0.06028447 -0.587752 0.28860852 -1.5711918 -0.46920586 0.70577353 -0.1825155 2.6019137 0.53800887 2.0388956 1.1267477 1.2491167 2.2639756 -0.695714 0.021000264 -0.018054683 -0.31883276 3.05128 -1.3045707 -2.8690743 0.042503554 -1.0368543 1.4625484 2.132626 -1.0482056 1.1042211 -1.8272645 0.92395014 1.3298126 2.0358117 -1.56781 0.16664277 -1.6970147 -0.44302478 0.013548814 -0.4594601 3.4404755 1.1110681 -0.60941976 0.6265386 0.80946475 -0.7507894 1.1109685 0.9413552 -1.5049429 2.0481293 1.9211582 -0.68767345 1.5065689 -0.5944365 -0.9077338 -2.9764507 0.8370789 0.81362617 -1.0928285 -1.3421642 -0.57118154 -1.2328687 -1.9633489 0.84809166 -1.4519063 -1.0918429 -1.868486 0.70797133 2.8786998 0.7862087 0.07976656 0.20497806 0.17032085 0.20717178'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_p[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "emb = []\n",
    "for line in w2v_p:\n",
    "    l = line.split()\n",
    "    word = l[0].split(\"_\")[0]\n",
    "    \n",
    "    vec = list(map(float, l[1:]))\n",
    "    \n",
    "    vocab += [word]\n",
    "    emb += [vec]\n",
    "\n",
    "emb = np.array(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189193"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189193, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5960777700118979"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(emb[vocab.index(\"красный\")], emb[vocab.index(\"зеленый\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19100238054656765"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(emb[vocab.index(\"красный\")], emb[vocab.index(\"человек\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using with ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [v.split()[0] for v in open(\"emb_dir/vocab.txt\").read().split(\"\\n\")]\n",
    "emb = np.load(\"emb_dir/emb.npy\")\n",
    "\n",
    "vocab = [\"<PAD>\", \"<UNK>\"] + vocab\n",
    "\n",
    "emb = np.insert(emb, 0, np.random.uniform(-1, 1, emb.shape[1]), axis=0)\n",
    "emb = np.insert(emb, 0, np.zeros(emb.shape[1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25942"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25942, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41583523,  0.50968206, -0.13879314, -0.16804345, -0.747004  ,\n",
       "       -0.91946787,  0.6386806 , -0.22171985,  0.50008035, -0.89180565,\n",
       "        0.61508083,  0.8735436 ,  0.36179537,  0.36583492, -0.11727881,\n",
       "        0.38155738, -0.56035167,  0.58727276,  0.56286174,  0.35117456,\n",
       "        0.87490153, -0.320434  , -0.10026085, -0.3016947 , -0.89546376,\n",
       "        0.07421842,  0.45389298, -0.86732966, -0.42485312,  0.51647836,\n",
       "        0.4295946 ,  0.6681166 , -0.3370198 ,  0.7573049 ,  0.16937827,\n",
       "        0.76184666,  0.97491103, -0.0215477 ,  0.5299386 , -0.19537358,\n",
       "        0.37821028, -0.788024  ,  0.22741619, -0.88069034, -0.16316468,\n",
       "        0.07166386,  0.73954093,  0.25548416,  0.38152096,  0.9114591 ,\n",
       "        0.34309018,  0.73081404, -0.78640765, -0.03425251,  0.42943048,\n",
       "       -0.28751266,  0.14582887, -0.7280036 , -0.97134376,  0.24407373,\n",
       "        0.3497146 , -0.18667412,  0.00170363, -0.22577421, -0.14483874,\n",
       "       -0.75114655,  0.75909424,  0.6838565 , -0.0205434 , -0.13243315,\n",
       "        0.23272282,  0.25671667,  0.53064233, -0.89619637, -0.9798971 ,\n",
       "       -0.6286246 , -0.7985787 ,  0.7802585 ,  0.07854921, -0.90839505,\n",
       "       -0.6257158 ,  0.02313144,  0.6615127 , -0.21709245, -0.07763366,\n",
       "       -0.4843687 , -0.01671509, -0.14707942,  0.45082745,  0.9537656 ,\n",
       "       -0.47504935, -0.3346658 , -0.70237136,  0.8690348 , -0.04344399,\n",
       "        0.87435776, -0.03353295,  0.62193596,  0.24349746, -0.36139014],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, delimiter, vocab, emb):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter, quotechar='\"')\n",
    "        next(reader) #skip csv header\n",
    "        for row in reader:\n",
    "            samples += [row[0]]\n",
    "            labels += [row[1]]\n",
    "\n",
    "    preprocessed_samples = []\n",
    "    for sample in samples:\n",
    "        s = sample.lower()\n",
    "        s = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", s)\n",
    "        s = re.sub(\"\\s+\", \" \", s)\n",
    "        s = s.strip()\n",
    "        preprocessed_samples += [s]\n",
    "    \n",
    "    tokenized_samples = []\n",
    "    for sample in preprocessed_samples:\n",
    "        s = sample.split()\n",
    "        tokenized_samples += [s]\n",
    "    \n",
    "    digitized_samples = []\n",
    "    for sample in tokenized_samples:\n",
    "        s = [vocab.index(token) if token in vocab else vocab.index(\"<UNK>\") for token in sample]\n",
    "            \n",
    "        digitized_samples += [s]\n",
    "        \n",
    "    correct_samples = []\n",
    "    max_len = 128\n",
    "\n",
    "    for sample in digitized_samples:\n",
    "        if len(sample) < max_len:\n",
    "            sample += [vocab.index(\"<PAD>\")] * (max_len - len(sample))\n",
    "\n",
    "        correct_samples += [sample[:max_len]]\n",
    "        \n",
    "    labels_dict = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "    correct_labels = [labels_dict[label] for label in labels]\n",
    "    \n",
    "    train_data = correct_samples[:4000]\n",
    "    train_labels = correct_labels[:4000]\n",
    "    test_data = correct_samples[4000:]\n",
    "    test_labels = correct_labels[4000:]\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_data('data/imdb/imdb_dataset.csv', ',', vocab, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python37264bitbasecondafefce09c800544f39fa4fc085bf429a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
